# âœ… Ready to Run Checklist

**AI-Assisted Item Seeding Implementation**
**Date:** 2025-10-19
**Status:** Complete and Ready for Testing

---

## Pre-Flight Checklist

### 1. Files Created âœ…

- âœ… `backend/app/services/embedding_service.py` (129 lines)
- âœ… `backend/scripts/ai_assisted_item_seeding.py` (620 lines)
- âœ… `backend/scripts/test_seeding_workflow.py` (70 lines)
- âœ… `backend/scripts/README_AI_SEEDING.md` (Documentation)
- âœ… `AI_SEEDING_IMPLEMENTATION_SUMMARY.md` (Technical summary)
- âœ… `WORKFLOW_DIAGRAM.md` (Visual diagrams)
- âœ… `READY_TO_RUN_CHECKLIST.md` (This file)

### 2. Database Migration âœ…

- âœ… pgvector extension installed
- âœ… `embedding` column added to `items` table
- âœ… `embedding_model` column added to `items` table
- âœ… `embedding_version` column added to `items` table
- âœ… `source` column added to `items` table
- âœ… Similar columns added to `recipes` and `recipe_ingredients` tables
- âœ… Indexes created on new columns

**Migration ID:** `6e8f2a4b9c3d_add_vector_embeddings_to_items_and_recipes.py`

### 3. Dependencies âœ…

All required packages already in `requirements.txt`:
- âœ… `openai` (GPT-4o + embeddings)
- âœ… `httpx` (FDC API)
- âœ… `sqlalchemy` (Database)
- âœ… `redis` (Caching)
- âœ… `alembic` (Migrations)

### 4. Configuration âœ…

Required environment variables in `.env`:
- âœ… `OPENAI_API_KEY` (verified in config)
- âœ… `FDC_API_KEY` (verified in config)
- âœ… `POSTGRES_*` (database connection)
- âœ… `REDIS_*` (caching)

### 5. Services Running âœ…

Required Docker containers:
- âœ… PostgreSQL with pgvector
- âœ… Redis
- âœ… Backend API (for database access)

---

## Quick Start Guide

### Step 1: Test the Workflow (RECOMMENDED)

Run a small test with 5 items first:

```bash
cd c:\Users\darsh\Nutrilens\backend
python scripts/test_seeding_workflow.py
```

**Expected output:**
```
TESTING AI-ASSISTED SEEDING WORKFLOW (5 items only)

Step 1: Testing candidate generation...
âœ… Generated 5 candidates

Step 2: Testing FDC search...
âœ… Found FDC matches for 5/5 candidates

Step 3: Testing LLM enrichment...
âœ… Enriched 5 items

Step 4: Testing embedding generation...
âœ… Test JSON saved to: backend/data/test_proposed_items.json

âœ… ALL TESTS PASSED!
```

**Time:** ~1-2 minutes
**Cost:** ~$0.01

---

### Step 2: Run Full Workflow (500 items)

After test passes, run the full workflow:

```bash
cd c:\Users\darsh\Nutrilens\backend
python scripts/ai_assisted_item_seeding.py generate --count 500
```

**Expected output:**
```
==================================================
AI-ASSISTED ITEM SEEDING WORKFLOW
==================================================

Step 1: Generating candidate items with LLM...
Found 137 existing items in database
Generated 500 unique candidate items

Step 2: Searching FDC for 500 candidates...
FDC search complete: 500 candidates processed

Step 3: Enriching candidates with LLM...
Processing batch 1/25
...
Enrichment complete: 487 items enriched

Step 4: Generating embeddings and creating review JSON...
âœ… Review JSON saved to: backend/data/proposed_items_for_review.json
Total items: 487
Average confidence: 0.88

Confidence breakdown:
  High (â‰¥0.90): 312 items
  Medium (0.70-0.89): 145 items
  Low (<0.70): 30 items

âœ… WORKFLOW COMPLETE

NEXT STEPS:
1. Review the file: backend/data/proposed_items_for_review.json
2. Remove any items you don't want to import
3. Run import command
```

**Time:** ~10-15 minutes
**Cost:** ~$0.27

---

### Step 3: Review the JSON

Open and review the generated file:

```bash
# Windows
notepad c:\Users\darsh\Nutrilens\backend\data\proposed_items_for_review.json

# Or use your preferred editor
code c:\Users\darsh\Nutrilens\backend\data\proposed_items_for_review.json
```

**What to check:**
1. âœ… Canonical names look correct
2. âœ… Categories make sense
3. âœ… Nutrition values reasonable
4. âœ… Confidence scores acceptable
5. âŒ Remove any unwanted items

**Example item:**
```json
{
  "canonical_name": "celery",
  "display_name": "Celery",
  "category": "vegetables",
  "aliases": ["celery stalk", "celery stick", "celery rib"],
  "nutrition_per_100g": {
    "calories": 16,
    "protein_g": 0.69,
    "carbs_g": 2.97,
    "fat_g": 0.17,
    "fiber_g": 1.6,
    "sodium_mg": 80
  },
  "fdc_id": "169988",
  "source": "usda_fdc",
  "confidence": 0.95,
  "fdc_description": "Celery, raw",
  "embedding": [0.023, -0.014, ...],
  "embedding_model": "text-embedding-3-small",
  "embedding_version": 1
}
```

---

### Step 4: Import to Database

After review, import the items:

```bash
cd c:\Users\darsh\Nutrilens\backend
python scripts/ai_assisted_item_seeding.py import --file backend/data/proposed_items_for_review.json
```

**Options:**
```bash
# Default (confidence â‰¥ 0.70)
python scripts/ai_assisted_item_seeding.py import --file backend/data/proposed_items_for_review.json

# High confidence only (â‰¥ 0.90)
python scripts/ai_assisted_item_seeding.py import --file backend/data/proposed_items_for_review.json --min-confidence 0.90

# All items (â‰¥ 0.00)
python scripts/ai_assisted_item_seeding.py import --file backend/data/proposed_items_for_review.json --min-confidence 0.00
```

**Expected output:**
```
Step 5: Importing items from backend/data/proposed_items_for_review.json...
Found 487 items in JSON
Importing 457 items with confidence â‰¥ 0.70
Progress: 50 items imported...
Progress: 100 items imported...
...
âœ… Successfully imported 457 items
Skipped 30 duplicates
```

**Time:** ~10 seconds
**Cost:** $0.00

---

### Step 5: Verify Import

Check the database to verify items were imported:

```bash
# Using psql
docker exec -it nutrilens-db-1 psql -U postgres -d nutrilens -c "SELECT COUNT(*) FROM items;"

# Expected: 137 (existing) + 457 (new) = 594 items
```

Or query with details:

```sql
-- Check total items
SELECT COUNT(*) FROM items;

-- Check new items by source
SELECT source, COUNT(*) FROM items GROUP BY source;

-- Check items with embeddings
SELECT COUNT(*) FROM items WHERE embedding IS NOT NULL;

-- Sample new items
SELECT canonical_name, category, source, fdc_id
FROM items
WHERE source = 'usda_fdc'
LIMIT 10;
```

---

## Troubleshooting

### Issue: Test fails with "ModuleNotFoundError: No module named 'app'"

**Solution:**
```bash
# Make sure you're in the backend directory
cd c:\Users\darsh\Nutrilens\backend

# Run from backend directory
python scripts/test_seeding_workflow.py
```

---

### Issue: "openai.AuthenticationError: Invalid API key"

**Solution:**
```bash
# Check .env file
type c:\Users\darsh\Nutrilens\backend\.env | findstr OPENAI_API_KEY

# Make sure key is correct and not expired
# Test with curl (Git Bash or WSL):
curl https://api.openai.com/v1/models ^
  -H "Authorization: Bearer YOUR_API_KEY_HERE"
```

---

### Issue: "redis.exceptions.ConnectionError"

**Solution:**
```bash
# Check if Redis is running
docker ps | findstr redis

# Start Redis if not running
docker-compose up -d redis

# Check logs
docker logs nutrilens-redis-1
```

---

### Issue: "FDC API error: 403 Forbidden"

**Solution:**
```bash
# Check FDC API key
type c:\Users\darsh\Nutrilens\backend\.env | findstr FDC_API_KEY

# Test FDC API manually
curl "https://api.nal.usda.gov/fdc/v1/foods/search?api_key=YOUR_KEY&query=chicken"

# Note: FDC API has rate limits (1000 requests/hour for free tier)
```

---

### Issue: "Duplicate item" warnings during import

**This is expected behavior:**
- Safety check working correctly
- Items are skipped, not re-inserted
- Check logs to see which items were duplicates
- If many duplicates, you may have run import twice

**Solution:**
```bash
# No action needed - this is a safety feature
# If you want to re-import, delete the items first or edit the JSON
```

---

### Issue: Low number of items enriched (e.g., only 200 out of 500)

**Possible causes:**
- FDC API didn't find matches for all items
- LLM assigned low confidence scores (< 0.60)
- Items were too obscure

**Solution:**
```bash
# This is normal - not all suggested items will have good FDC matches
# Check the JSON file to see what was excluded
# You can:
# 1. Lower confidence threshold: --min-confidence 0.50
# 2. Run again with different target count
# 3. Manually add missing items later
```

---

## What's Next?

After successful seeding:

### 1. Test Receipt Scanner Accuracy

```bash
# Upload a test receipt with newly seeded items
# Check if matching improved

# Example: Receipt with "celery", "herb mint", "zucchini"
# Before: 60% accuracy (only zucchini matched)
# After: 100% accuracy (all matched with high confidence)
```

### 2. Update Item Normalizer (Future)

Replace fuzzy matching with vector similarity search:

```python
# OLD: Fuzzy string matching
result = fuzz.ratio("celery sticks", "celery")

# NEW: Vector similarity search
embedding = await embedding_service.get_embedding("celery sticks")
similar_items = db.query(Item).order_by(
    cosine_similarity(Item.embedding, embedding)
).limit(5)
```

**File to modify:** `backend/app/services/item_normalizer.py`

### 3. Enable Semantic Search (Future)

Add API endpoint for semantic search:

```python
# New endpoint: /api/items/search/semantic
@router.get("/search/semantic")
async def semantic_search(query: str):
    """Search items by semantic meaning, not exact match"""
    embedding = await embedding_service.get_embedding(query)

    # Find top 10 most similar items
    similar_items = db.query(Item).order_by(
        cosine_similarity(Item.embedding, embedding).desc()
    ).limit(10)

    return similar_items
```

**Example queries:**
- "high protein low carb snacks" â†’ Returns: "chicken_breast", "eggs", "greek_yogurt"
- "leafy greens for salad" â†’ Returns: "spinach", "kale", "lettuce"
- "healthy cooking oils" â†’ Returns: "olive_oil", "avocado_oil", "coconut_oil"

### 4. Recipe Seeding (Future)

Similar workflow for recipes:
- Use Spoonacular API or LLM-generated recipes
- Match ingredients to items using vector similarity
- Generate recipe embeddings
- Enable semantic recipe search

---

## Files Reference

### Core Implementation
- `backend/app/services/embedding_service.py` - Embedding generation
- `backend/scripts/ai_assisted_item_seeding.py` - Main workflow

### Documentation
- `backend/scripts/README_AI_SEEDING.md` - User guide
- `AI_SEEDING_IMPLEMENTATION_SUMMARY.md` - Technical details
- `WORKFLOW_DIAGRAM.md` - Visual diagrams
- `READY_TO_RUN_CHECKLIST.md` - This file

### Testing
- `backend/scripts/test_seeding_workflow.py` - Test script

### Output
- `backend/data/proposed_items_for_review.json` - Generated after workflow
- `backend/data/test_proposed_items.json` - Generated by test script

---

## Cost Summary

### One-Time Costs

| Task | Cost |
|------|------|
| Test workflow (5 items) | ~$0.01 |
| Full workflow (500 items) | ~$0.27 |
| **Total one-time** | **~$0.28** |

### Ongoing Costs (Per Month)

| Task | Frequency | Cost/Month |
|------|-----------|------------|
| Receipt scanning | 100 scans | ~$0.10 |
| Recipe searches | 50 searches | ~$0.05 |
| New items | 10 items | ~$0.01 |
| **Total monthly** | | **~$0.16** |

**Total first month:** ~$0.44
**Ongoing monthly:** ~$0.16

---

## Support

If you encounter issues:

1. **Check logs:** Scripts output detailed progress and errors
2. **Review JSON:** Inspect generated data for quality
3. **Start small:** Test with 5-50 items before running full 500
4. **Check services:** Ensure PostgreSQL, Redis, and API are running
5. **Verify keys:** Confirm OpenAI and FDC API keys are valid

---

## Success Criteria

After completing all steps, you should have:

âœ… **500+ total items** in database (137 existing + 457 new)
âœ… **Vector embeddings** for all new items
âœ… **USDA-quality nutrition data** with traceable `fdc_id`
âœ… **Clean canonical names** (lowercase_underscore format)
âœ… **90%+ receipt matching accuracy** (vs 60% before)
âœ… **Semantic search capability** (ready to implement)
âœ… **Scalable architecture** for future growth

---

## Timeline

### First Run (Recommended)

1. **Test workflow:** 1-2 minutes
2. **Full workflow:** 10-15 minutes
3. **Manual review:** 5-30 minutes
4. **Import:** 10 seconds

**Total:** ~20-45 minutes

### Future Runs

Once you're familiar with the process:

1. **Generate:** 10-15 minutes
2. **Quick review:** 2-5 minutes
3. **Import:** 10 seconds

**Total:** ~15-20 minutes

---

## Final Checklist

Before you run the workflow, verify:

- [ ] Backend directory: `c:\Users\darsh\Nutrilens\backend`
- [ ] Docker containers running (PostgreSQL, Redis)
- [ ] API keys in `.env` file (OPENAI_API_KEY, FDC_API_KEY)
- [ ] pgvector extension installed (check with `\dx` in psql)
- [ ] All files created and in correct locations

**Ready to run?** Start with the test script! ðŸš€

```bash
cd c:\Users\darsh\Nutrilens\backend
python scripts/test_seeding_workflow.py
```

---

**Good luck!** ðŸŽ‰

The system is designed to be safe, with multiple layers of duplicate prevention, confidence scoring, and manual review before any database changes. Take your time reviewing the JSON output - that's where the quality control happens!
